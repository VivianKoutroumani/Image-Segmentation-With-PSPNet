{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://upload.wikimedia.org/wikipedia/fr/7/78/Logo_nouveau_CentraleSupelecParisSaclay.jpg' width=200></center>\n",
        "\n",
        "<h6><center>M.Sc. DSBA</center></h6>\n",
        "<h3><center>Deep learning Kaggle Competition</center></h3>\n",
        "\n",
        "\n",
        "<h1>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "<center>Segment features around residential buildings in UAV images of flooded area taken in Houston after Hurrican Harvey.</center>\n",
        "<hr style=\" border:none; height:3px;\">\n",
        "</h1>\n"
      ],
      "metadata": {
        "id": "75qpg2lntLTl"
      },
      "id": "75qpg2lntLTl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Packages and load libraries\n"
      ],
      "metadata": {
        "id": "xA9dhIetKq_L"
      },
      "id": "xA9dhIetKq_L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Specific versions of some packages were needed for the model to run"
      ],
      "metadata": {
        "id": "0F2nye1Yqmmx"
      },
      "id": "0F2nye1Yqmmx"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install utils\n",
        "!pip install segmentation_models_pytorch\n",
        "!pip install opencv-python-headless==4.1.2.30\n",
        "!pip install --force-reinstall numpy==1.18.5\n",
        "!pip install --force-reinstall folium==0.2.1\n",
        "!pip install --force-reinstall Jinja2==2.10.1\n",
        "!pip install -U albumentations>=0.3.0 --user\n",
        "!pip install -U git+https://github.com/albu/albumentations --no-cache-dir\n",
        "!pip install --upgrade albumentations\n",
        "!pip install --upgrade git+https://github.com/albumentations-team/albumentations\n",
        "!pip install --force-reinstall albumentations==1.0.3"
      ],
      "metadata": {
        "id": "589DEa00IKLM"
      },
      "id": "589DEa00IKLM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now restart the runtime"
      ],
      "metadata": {
        "id": "FtkNG2ayl2Vz"
      },
      "id": "FtkNG2ayl2Vz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d840d42a",
      "metadata": {
        "id": "d840d42a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from utils import *\n",
        "import albumentations as album\n",
        "import segmentation_models_pytorch as smp\n",
        "sns.set(style='white', context='notebook', palette='deep')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "6--fd69fKzDI"
      },
      "id": "6--fd69fKzDI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c67726",
      "metadata": {
        "id": "97c67726"
      },
      "outputs": [],
      "source": [
        "def keep_image_size_open(path, size=(512, 512)):\n",
        "    img = Image.open(path)\n",
        "    side = max(img.size)  # Get the longest side of the image\n",
        "    mask = Image.new('RGB', (side, side), (0, 0, 0))  # Create a square canvas\n",
        "    mask.paste(img, (0, 0))  # Paste the original image on the left top of the canvas\n",
        "    mask = mask.resize(size)  # Resize the new image to a uniform size\n",
        "    return mask\n",
        "\n",
        "def keep_mask_size_open(path, size=(512, 512)):\n",
        "    img = Image.open(path)\n",
        "    side = max(img.size)  # Get the longest side of the image\n",
        "    mask = Image.new('L', (side, side), 0)  # Create a square canvas\n",
        "    mask.paste(img, (0, 0))  # Paste the original image on the left top of the canvas\n",
        "    mask = mask.resize(size)  # Resize the new image to a uniform size\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d6669a4",
      "metadata": {
        "id": "1d6669a4"
      },
      "outputs": [],
      "source": [
        "# helper function for data visualization\n",
        "def visualize(**images):\n",
        "    \"\"\"\n",
        "    Plot images in one row\n",
        "    \"\"\"\n",
        "    n_images = len(images)\n",
        "    plt.figure(figsize=(20,8))\n",
        "    for idx, (name, image) in enumerate(images.items()):\n",
        "        plt.subplot(1, n_images, idx + 1)\n",
        "        plt.xticks([]); \n",
        "        plt.yticks([])\n",
        "        # get title from the parameter names\n",
        "        plt.title(name.replace('_',' ').title(), fontsize=20)\n",
        "        plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "# Perform one hot encoding on label\n",
        "def one_hot_encode(image,n_classes):\n",
        "    x = F.one_hot(image,n_classes)\n",
        "    return x\n",
        "     \n",
        "# Perform reverse one-hot-encoding on labels / preds\n",
        "def reverse_one_hot(image):\n",
        "    x = np.argmax(image, axis = -1)\n",
        "    return x\n",
        "\n",
        "# Perform colour coding on the reverse-one-hot outputs\n",
        "def colour_code_segmentation(image, label_values):\n",
        "    colour_codes = np.array(label_values)\n",
        "    x = colour_codes[image.astype(int)]\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d528ab25",
      "metadata": {
        "id": "d528ab25"
      },
      "outputs": [],
      "source": [
        "def get_training_augmentation():\n",
        "    train_transform = [\n",
        "        album.PadIfNeeded(min_height=512, min_width=512, always_apply=True, border_mode=0),\n",
        "        album.OneOf([album.HorizontalFlip(p=1),album.VerticalFlip(p=1),album.RandomRotate90(p=1),],p=0.5),\n",
        "        album.ShiftScaleRotate(scale_limit=0.5,rotate_limit=0,shift_limit=0.1,p=0.5,border_mode=0), \n",
        "        album.GridDistortion(p=0.5)\n",
        "    ]\n",
        "    return album.Compose(train_transform)\n",
        "\n",
        "\n",
        "def get_validation_augmentation():\n",
        "    # Add sufficient padding to ensure image is divisible by 32\n",
        "    test_transform = [\n",
        "        album.PadIfNeeded(min_height=512, min_width=512, always_apply=True, border_mode=0),\n",
        "    ]\n",
        "    return album.Compose(test_transform)\n",
        "\n",
        "\n",
        "def to_tensor(x, **kwargs):\n",
        "    return x.transpose(2,0,1).astype('float32')\n",
        "\n",
        "\n",
        "def get_preprocessing(preprocessing_fn=None):  \n",
        "    _transform = [\n",
        "        album.Lambda(image=preprocessing_fn),\n",
        "        album.Lambda(image=to_tensor, mask=to_tensor),\n",
        "    ]\n",
        "    return album.Compose(_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78fa2f6e",
      "metadata": {
        "id": "78fa2f6e"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "class BackgroundDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "            self,path1, path2, \n",
        "            augmentation=None, \n",
        "            preprocessing=None,\n",
        "    ):\n",
        "        self.path1 = path1\n",
        "        self.path2 = path2\n",
        "        self.name = os.listdir(os.path.join(path2, 'train_masks'))\n",
        "        self.augmentation = augmentation\n",
        "        self.preprocessing = preprocessing\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        # read images and masks\n",
        "        mask_name = self.name[idx]\n",
        "        mask_path = os.path.join(self.path2,'train_masks',mask_name)\n",
        "        img_path = os.path.join(self.path1,'train_images',mask_name.replace('png','jpg'))\n",
        "\n",
        "        \n",
        "        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.cvtColor(cv2.imread(mask_path),0)\n",
        "        image = keep_image_size_open(img_path)\n",
        "        mask = keep_mask_size_open(mask_path)\n",
        "        \n",
        "        image = np.asarray(image).astype('int64')\n",
        "        mask = np.asarray(mask).astype('int64')\n",
        "        \n",
        "        # one-hot-encode the mask  \n",
        "        mask = torch.from_numpy(mask).to(torch.int64)\n",
        "        mask = one_hot_encode(mask,27)\n",
        "               \n",
        "         #Augmentation\n",
        "        mask = np.asarray(mask).astype('int64')\n",
        "        sample = self.augmentation(image=image, mask=mask)\n",
        "        image, mask = sample['image'], sample['mask']\n",
        "        \n",
        "      \n",
        "         # preprocessing applied only on numpy array image\n",
        "        sample = self.preprocessing(image=image, mask=mask)\n",
        "        image, mask = sample['image'], sample['mask']\n",
        "                  \n",
        "        return image,mask\n",
        "        \n",
        "    def __len__(self):\n",
        "        # return length of \n",
        "        return len(self.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoaders"
      ],
      "metadata": {
        "id": "iUN-3ntLsiPS"
      },
      "id": "iUN-3ntLsiPS"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB9PoREKJDCR",
        "outputId": "9895c7c1-93f1-4592-8775-82a6d2238968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "id": "iB9PoREKJDCR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57a7d352",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57a7d352",
        "outputId": "e037b51e-0b0b-4874-bc1c-9001deacf8cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 512, 512) (27, 512, 512)\n",
            "float32 float32\n",
            "261\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    data = BackgroundDataset('/content/drive/MyDrive/fdl21-fdl-dsba/train_images',\n",
        "                             '/content/drive/MyDrive/fdl21-fdl-dsba/train_masks',\n",
        "                             augmentation=get_training_augmentation(),preprocessing=get_preprocessing(preprocessing_fn))\n",
        "    check_image = data[35][0] # checking for the random 35th image\n",
        "    check_mask = data[10|0][1]\n",
        "    print(check_image.shape,check_mask.shape)\n",
        "    print(check_image.dtype,check_mask.dtype)\n",
        "    print(len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48676020",
      "metadata": {
        "id": "48676020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fefdbd9-ce5f-4333-f549-07949d08e085"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ],
      "source": [
        "batch_size = 10\n",
        "num_workers = 4\n",
        "# Splitting into Train and Val\n",
        "full_dataset = BackgroundDataset('/content/drive/MyDrive/fdl21-fdl-dsba/train_images',\n",
        "                             '/content/drive/MyDrive/fdl21-fdl-dsba/train_masks',\n",
        "                             augmentation=get_training_augmentation(),preprocessing=get_preprocessing(preprocessing_fn))\n",
        "train_size = int(0.9 * len(full_dataset))\n",
        "val_size   = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Creating  data_loader\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,num_workers=num_workers,shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size,num_workers=num_workers,shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a585a878",
      "metadata": {
        "id": "a585a878"
      },
      "outputs": [],
      "source": [
        "it, lt = next(iter(train_loader))\n",
        "print(it.shape,lt.shape)\n",
        "print(it.dtype,lt.dtype)\n",
        "\n",
        "print(len(train_loader)*batch_size,len(val_loader)*batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model set"
      ],
      "metadata": {
        "id": "i5tU9_mDsX_g"
      },
      "id": "i5tU9_mDsX_g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B2BOjalq1lT"
      },
      "outputs": [],
      "source": [
        "# Need to be run only one time\n",
        "ENCODER = 'resnext50_32x4d'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "CLASSES = 27\n",
        "ACTIVATION = 'softmax2d' #'softmax2d' for multiclass segmentation\n",
        "\n",
        "# create segmentation model with pretrained encoder\n",
        "model = smp.PSPNet(\n",
        "    encoder_name=ENCODER, \n",
        "    encoder_weights=ENCODER_WEIGHTS, \n",
        "    classes=27,  \n",
        "    activation=ACTIVATION\n",
        ")\n",
        "\n",
        "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
      ],
      "id": "8B2BOjalq1lT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37ef0185",
      "metadata": {
        "id": "37ef0185"
      },
      "outputs": [],
      "source": [
        "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
        "TRAINING = True\n",
        "\n",
        "# Set num of epochs\n",
        "EPOCHS = 20\n",
        "\n",
        "# Set device: `cuda` or `cpu`\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# define loss function\n",
        "loss = smp.utils.losses.CrossEntropyLoss()\n",
        "\n",
        "# define metrics\n",
        "metrics = [\n",
        "    smp.utils.metrics.IoU(threshold=0.5),\n",
        "]\n",
        "\n",
        "\n",
        "optimizer = torch.optim.SGD([ \n",
        "    dict(params=model.parameters(), lr=0.003),\n",
        "])\n",
        "# define learning rate scheduler (not used in this NB)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n",
        ")\n",
        "\n",
        "if os.path.exists('../input/pyramid-scene-parsing-pspnet-resnext50-pytorch/best_model.pth'):\n",
        "    model = torch.load('../input/pyramid-scene-parsing-pspnet-resnext50-pytorch/best_model.pth', map_location=DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "MkTEf2g8rIra"
      },
      "id": "MkTEf2g8rIra"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e309d038",
      "metadata": {
        "id": "e309d038"
      },
      "outputs": [],
      "source": [
        "train_epoch = smp.utils.train.TrainEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    optimizer=optimizer,\n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "valid_epoch = smp.utils.train.ValidEpoch(\n",
        "    model, \n",
        "    loss=loss, \n",
        "    metrics=metrics, \n",
        "    device=DEVICE,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e44b02c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e44b02c0",
        "outputId": "7888f5d0-a8eb-4af1-f2b9-bcbcb7c99760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 0\n",
            "train:   0%|          | 0/24 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 100%|██████████| 24/24 [13:57<00:00, 34.91s/it, cross_entropy_loss - 3.303, iou_score - 0.0001211]\n",
            "valid: 100%|██████████| 3/3 [00:45<00:00, 15.22s/it, cross_entropy_loss - 3.298, iou_score - 4.36e-14]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 1\n",
            "train: 100%|██████████| 24/24 [14:20<00:00, 35.84s/it, cross_entropy_loss - 3.269, iou_score - 0.01628]\n",
            "valid: 100%|██████████| 3/3 [00:46<00:00, 15.65s/it, cross_entropy_loss - 3.245, iou_score - 0.03187]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 2\n",
            "train: 100%|██████████| 24/24 [14:11<00:00, 35.49s/it, cross_entropy_loss - 3.197, iou_score - 0.1041]\n",
            "valid: 100%|██████████| 3/3 [00:45<00:00, 15.21s/it, cross_entropy_loss - 3.15, iou_score - 0.1451]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 3\n",
            "train: 100%|██████████| 24/24 [14:30<00:00, 36.28s/it, cross_entropy_loss - 3.124, iou_score - 0.1493]\n",
            "valid: 100%|██████████| 3/3 [00:47<00:00, 15.85s/it, cross_entropy_loss - 3.015, iou_score - 0.2397]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 4\n",
            "train: 100%|██████████| 24/24 [14:17<00:00, 35.71s/it, cross_entropy_loss - 2.967, iou_score - 0.2872]\n",
            "valid: 100%|██████████| 3/3 [00:45<00:00, 15.16s/it, cross_entropy_loss - 2.91, iou_score - 0.3287]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 5\n",
            "train: 100%|██████████| 24/24 [14:24<00:00, 36.00s/it, cross_entropy_loss - 2.916, iou_score - 0.3241]\n",
            "valid: 100%|██████████| 3/3 [00:48<00:00, 16.16s/it, cross_entropy_loss - 2.889, iou_score - 0.3366]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 6\n",
            "train: 100%|██████████| 24/24 [14:28<00:00, 36.19s/it, cross_entropy_loss - 2.901, iou_score - 0.3288]\n",
            "valid: 100%|██████████| 3/3 [00:46<00:00, 15.48s/it, cross_entropy_loss - 2.875, iou_score - 0.3407]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 7\n",
            "train: 100%|██████████| 24/24 [14:27<00:00, 36.16s/it, cross_entropy_loss - 2.89, iou_score - 0.332]\n",
            "valid: 100%|██████████| 3/3 [00:48<00:00, 16.07s/it, cross_entropy_loss - 2.868, iou_score - 0.344]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 8\n",
            "train: 100%|██████████| 24/24 [14:24<00:00, 36.01s/it, cross_entropy_loss - 2.881, iou_score - 0.3366]\n",
            "valid: 100%|██████████| 3/3 [00:46<00:00, 15.46s/it, cross_entropy_loss - 2.86, iou_score - 0.3499]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 9\n",
            "train: 100%|██████████| 24/24 [14:36<00:00, 36.51s/it, cross_entropy_loss - 2.877, iou_score - 0.3397]\n",
            "valid: 100%|██████████| 3/3 [00:47<00:00, 15.97s/it, cross_entropy_loss - 2.854, iou_score - 0.3543]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 10\n",
            "train: 100%|██████████| 24/24 [14:22<00:00, 35.95s/it, cross_entropy_loss - 2.875, iou_score - 0.3382]\n",
            "valid: 100%|██████████| 3/3 [00:47<00:00, 15.69s/it, cross_entropy_loss - 2.849, iou_score - 0.3553]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 11\n",
            "train: 100%|██████████| 24/24 [14:33<00:00, 36.39s/it, cross_entropy_loss - 2.869, iou_score - 0.3417]\n",
            "valid: 100%|██████████| 3/3 [00:49<00:00, 16.39s/it, cross_entropy_loss - 2.843, iou_score - 0.3609]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 12\n",
            "train: 100%|██████████| 24/24 [14:26<00:00, 36.10s/it, cross_entropy_loss - 2.865, iou_score - 0.3445]\n",
            "valid: 100%|██████████| 3/3 [00:46<00:00, 15.40s/it, cross_entropy_loss - 2.84, iou_score - 0.3642]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 13\n",
            "train: 100%|██████████| 24/24 [14:43<00:00, 36.83s/it, cross_entropy_loss - 2.861, iou_score - 0.3476]\n",
            "valid: 100%|██████████| 3/3 [00:48<00:00, 16.22s/it, cross_entropy_loss - 2.85, iou_score - 0.3534]\n",
            "\n",
            "Epoch: 14\n",
            "train: 100%|██████████| 24/24 [14:30<00:00, 36.28s/it, cross_entropy_loss - 2.861, iou_score - 0.3483]\n",
            "valid: 100%|██████████| 3/3 [00:45<00:00, 15.12s/it, cross_entropy_loss - 2.834, iou_score - 0.3702]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 15\n",
            "train: 100%|██████████| 24/24 [14:27<00:00, 36.14s/it, cross_entropy_loss - 2.857, iou_score - 0.3493]\n",
            "valid: 100%|██████████| 3/3 [00:48<00:00, 16.17s/it, cross_entropy_loss - 2.836, iou_score - 0.3672]\n",
            "\n",
            "Epoch: 16\n",
            "train: 100%|██████████| 24/24 [14:22<00:00, 35.96s/it, cross_entropy_loss - 2.856, iou_score - 0.3499]\n",
            "valid: 100%|██████████| 3/3 [00:45<00:00, 15.24s/it, cross_entropy_loss - 2.833, iou_score - 0.3696]\n",
            "\n",
            "Epoch: 17\n",
            "train: 100%|██████████| 24/24 [14:36<00:00, 36.52s/it, cross_entropy_loss - 2.85, iou_score - 0.3545]\n",
            "valid: 100%|██████████| 3/3 [00:48<00:00, 16.19s/it, cross_entropy_loss - 2.828, iou_score - 0.3737]\n",
            "Model saved!\n",
            "\n",
            "Epoch: 18\n",
            "train: 100%|██████████| 24/24 [14:21<00:00, 35.92s/it, cross_entropy_loss - 2.85, iou_score - 0.3537]\n",
            "valid: 100%|██████████| 3/3 [00:45<00:00, 15.29s/it, cross_entropy_loss - 2.829, iou_score - 0.3736]\n",
            "\n",
            "Epoch: 19\n",
            "train: 100%|██████████| 24/24 [14:35<00:00, 36.48s/it, cross_entropy_loss - 2.848, iou_score - 0.356]\n",
            "valid: 100%|██████████| 3/3 [00:47<00:00, 15.96s/it, cross_entropy_loss - 2.827, iou_score - 0.3714]\n",
            "CPU times: user 4h 23min 59s, sys: 20min 30s, total: 4h 44min 30s\n",
            "Wall time: 5h 4min 33s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "if TRAINING:\n",
        "\n",
        "    best_iou_score = 0.0\n",
        "    train_logs_list, valid_logs_list = [], []\n",
        "\n",
        "    for i in range(0, EPOCHS):\n",
        "\n",
        "        # Perform training & validation\n",
        "        print('\\nEpoch: {}'.format(i))\n",
        "        train_logs = train_epoch.run(train_loader)\n",
        "        valid_logs = valid_epoch.run(val_loader)\n",
        "        train_logs_list.append(train_logs)\n",
        "        valid_logs_list.append(valid_logs)\n",
        "\n",
        "        # Save model if a better val IoU score is obtained\n",
        "        if best_iou_score < valid_logs['iou_score']:\n",
        "            best_iou_score = valid_logs['iou_score']\n",
        "            torch.save(model, '/content/drive/MyDrive/fdl21-fdl-dsba/best_model_psnet3.pt')\n",
        "            print('Model saved!')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c498bc21"
      },
      "outputs": [],
      "source": [
        "PATH = '/content/drive/MyDrive/fdl21-fdl-dsba/best_model_psnet3.pt'\n",
        "device = torch.device('cpu')\n",
        "model = torch.load(PATH, map_location=torch.device('cpu'))\n",
        "model.eval();"
      ],
      "id": "c498bc21"
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "KkpW5saa-fO6"
      },
      "id": "KkpW5saa-fO6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nVecTM2jtJW_"
      },
      "id": "nVecTM2jtJW_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test set loading and submission file"
      ],
      "metadata": {
        "id": "j7V1VeuKrQuv"
      },
      "id": "j7V1VeuKrQuv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c2f5b25"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        self.name = os.listdir(os.path.join(path, 'test_images'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.name)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_name = self.name[index]\n",
        "        img_path = os.path.join(self.path, 'test_images',img_name)\n",
        "        assert os.path.isfile(img_path)\n",
        "        img = Image.open(img_path)\n",
        "        img = img.resize((512,512))\n",
        "        img = transform(img)\n",
        "        \n",
        "        return img"
      ],
      "id": "8c2f5b25"
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TestDataset('/content/drive/MyDrive/fdl21-fdl-dsba/test_images')\n",
        "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
        "it = next(iter(test_loader))\n",
        "print(it.shape,it.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhJ2vu1H3xhg",
        "outputId": "5fdbe5ef-9f44-48d6-bc36-71000a63a5f1"
      },
      "id": "BhJ2vu1H3xhg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 512, 512]) torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7a5ace9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3cb0ec57d9ad4504aab68becb20ee349",
            "4304c05558a348faa44bed6e83bd51d3",
            "37cd162e7b69408eb16568f67b5501b1",
            "7e1210d360814370a9e697c91d5c4d7d",
            "75f1f70c334642dd81618f757545d8c6",
            "b351ebed5f40470f9b7fe4f1cb279bf9",
            "220c97bd5bf544dd877e04cb5be35e7a",
            "c9671a22ed7c4ff595d5888df8d68ff0",
            "fc5d2250854a42b3bd978b7f8702955d",
            "0b9fb988239d4531868eaa8c14749b90",
            "b45e431d6f764fa78f7396fa411381ed"
          ]
        },
        "outputId": "e9ccdfd1-2b6f-4b26-c8dd-f84e6341d62c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3cb0ec57d9ad4504aab68becb20ee349",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/56 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n",
            "(2, 27, 512, 512)\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm as tq\n",
        "test_mask=[]\n",
        "for data in tq(test_loader):\n",
        "    output = model(data).cpu().detach().numpy()\n",
        "    print(output.shape)\n",
        "    for b in range(output.shape[0]):\n",
        "                   test_mask.append(output[b])\n"
      ],
      "id": "a7a5ace9"
    },
    {
      "cell_type": "code",
      "source": [
        "mask[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32k_gcABD5Nt",
        "outputId": "dfb8830f-c2c8-4395-ffe4-e086d8c7a1e0"
      },
      "id": "32k_gcABD5Nt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512, 512)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58c88edf"
      },
      "outputs": [],
      "source": [
        "tarr = []\n",
        "for x in os.listdir('test_images'):\n",
        "    imp = os.path.join('test_images',x)\n",
        "    img = Image.open(imp)\n",
        "    i = np.array(img)\n",
        "    tarr.append(i)"
      ],
      "id": "58c88edf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a10a60cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef21db1-57d2-45f2-b876-2e92a9737d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n"
          ]
        }
      ],
      "source": [
        "path1 = '/content/drive/MyDrive/fdl21-fdl-dsba/test_masks3'\n",
        "path2 = '/content/drive/MyDrive/fdl21-fdl-dsba/test_images/test_images'\n",
        "file_name = list(os.listdir(path2))\n",
        "for i in range(len(mask)):\n",
        "    mask_name = file_name[i]\n",
        "    mask_name = mask_name.replace('jpg','png')\n",
        "    file_path = os.path.join(path1,mask_name)\n",
        "    img = Image.fromarray(mask[i].astype('uint8'))\n",
        "    img.save(file_path)\n",
        "    print(i)"
      ],
      "id": "a10a60cc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8afcfc6f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def rle_encode(img):\n",
        "    '''\n",
        "    img: numpy array, 1 - mask, 0 - background\n",
        "    Returns run length as string formated\n",
        "    '''\n",
        "    pixels = img.flatten()\n",
        "    pixels = np.concatenate([[0], pixels, [0]])\n",
        "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
        "    runs[1::2] -= runs[::2]\n",
        "    return ' '.join(str(x) for x in runs)\n",
        "\n",
        "def rle_decode(mask_rle, shape):\n",
        "    '''\n",
        "    mask_rle: run-length as string formated (start length)\n",
        "    shape: (height,width) of array to return\n",
        "    Returns numpy array, 1 - mask, 0 - background\n",
        "    '''\n",
        "    s = mask_rle.split()\n",
        "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
        "    starts -= 1\n",
        "    ends = starts + lengths\n",
        "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
        "    for lo, hi in zip(starts, ends):\n",
        "        img[lo:hi] = 1\n",
        "    return img.reshape(shape)\n",
        "\n",
        "\n",
        "def create_rles():\n",
        "    \"\"\"Used for Kaggle submission: predicts and encode all test images\"\"\"\n",
        "    dir = '/content/drive/MyDrive/fdl21-fdl-dsba/test_masks3/'\n",
        "    N = len(list(os.listdir(dir)))\n",
        "    with open('submission_file_pspnet3.csv', 'w') as f:\n",
        "        f.write('ImageClassId,rle_mask\\n')\n",
        "        for index, i in enumerate(os.listdir(dir)):\n",
        "            # print('{}/{}'.format(index, N))\n",
        "\n",
        "            mask = Image.open(dir+i)\n",
        "            mask = mask.resize((1024, 1024), resample=Image.NEAREST)\n",
        "            mask = np.array(mask)\n",
        "\n",
        "            for x in range(1, 25):\n",
        "                enc = rle_encode(mask == x)\n",
        "                f.write(f\"{i.split('_')[0]}_{x},{enc}\\n\")\n",
        "\n",
        "create_rles()"
      ],
      "id": "8afcfc6f"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Kaggle_final_code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3cb0ec57d9ad4504aab68becb20ee349": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4304c05558a348faa44bed6e83bd51d3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_37cd162e7b69408eb16568f67b5501b1",
              "IPY_MODEL_7e1210d360814370a9e697c91d5c4d7d",
              "IPY_MODEL_75f1f70c334642dd81618f757545d8c6"
            ]
          }
        },
        "4304c05558a348faa44bed6e83bd51d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37cd162e7b69408eb16568f67b5501b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b351ebed5f40470f9b7fe4f1cb279bf9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_220c97bd5bf544dd877e04cb5be35e7a"
          }
        },
        "7e1210d360814370a9e697c91d5c4d7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c9671a22ed7c4ff595d5888df8d68ff0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 56,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 56,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fc5d2250854a42b3bd978b7f8702955d"
          }
        },
        "75f1f70c334642dd81618f757545d8c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0b9fb988239d4531868eaa8c14749b90",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 56/56 [02:20&lt;00:00,  2.39s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b45e431d6f764fa78f7396fa411381ed"
          }
        },
        "b351ebed5f40470f9b7fe4f1cb279bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "220c97bd5bf544dd877e04cb5be35e7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c9671a22ed7c4ff595d5888df8d68ff0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fc5d2250854a42b3bd978b7f8702955d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0b9fb988239d4531868eaa8c14749b90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b45e431d6f764fa78f7396fa411381ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}